{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IAPR 2020:][iapr2020] Lab 3 ‒  Classification\n",
    "\n",
    "**Authors:** Sven Borden, Sorya Jullien, Artur Jesslen  \n",
    "**Due date:** 03.05.2020\n",
    "\n",
    "[iapr2020]: https://github.com/LTS5/iapr-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before running this notebook, verify that you have all requirements by running this command in your terminal:\n",
    "`pip install numpy pandas scipy sympy tensorflow matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "In this part, we will study classification based on the data available in the Matlab file `classification.mat` that you will under `lab-03-data/part1`.\n",
    "There are 3 data sets in this file, each one being a training set for a given class.\n",
    "They are contained in variables `a`, `b` and `c`.\n",
    "\n",
    "**Note**: we can load Matlab files using the [scipy.io] module.\n",
    "\n",
    "[scipy.io]: https://docs.scipy.org/doc/scipy/reference/io.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract relevant data\n",
    "We first need to extract the `lab-03-data.tar.gz` archive.\n",
    "To this end, we use the [tarfile] module from the Python standard library.\n",
    "\n",
    "[tarfile]: https://docs.python.org/3.6/library/tarfile.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import scipy.io\n",
    "from scipy.stats import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Sympy modules\n",
    "from sympy import symbols\n",
    "from sympy import plot_implicit\n",
    "from sympy import Eq\n",
    "from sympy.solvers import solve\n",
    "from mpmath import log\n",
    "from sympy import Matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "data_base_path = os.path.join(os.pardir, 'data')\n",
    "data_folder = 'lab-03-data'\n",
    "tar_path = os.path.join(data_base_path, data_folder + '.tar.gz')\n",
    "with tarfile.open(tar_path, mode='r:gz') as tar:\n",
    "    tar.extractall(path=data_base_path)\n",
    "    \n",
    "data_part1_path = os.path.join(data_base_path, data_folder, 'part1', 'classification.mat')\n",
    "matfile = scipy.io.loadmat(data_part1_path)\n",
    "a = matfile['a']\n",
    "b = matfile['b']\n",
    "c = matfile['c']\n",
    "\n",
    "print(a.shape, b.shape, c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Bayes method\n",
    "Using the Bayes method, give the analytical expression of the separation curves between those three classes.\n",
    "Do reasonable hypotheses about the distributions of those classes and estimate the corresponding parameters based on the given training sets.\n",
    "Draw those curves on a plot, together with the training data.\n",
    "For simplicity reasons, round the estimated parameters to the closest integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_colors = ['red', 'green', 'blue'] #used later for plotting colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot more easily n by 2 matrix (n vectors)\n",
    "def separateXY(matrix):\n",
    "    return matrix[:,0], matrix[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot raw classes\n",
    "plt.figure()\n",
    "for label_, cl, color_ in zip(['a','b','c'],[a, b, c], class_colors):\n",
    "    x, y = separateXY(cl)\n",
    "    plt.scatter(x, y, label=label_, color=color_)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we can tell that the distributions for each class is reasonably gaussian. We also can see than each class is balanced in the number of samples. To quantify how much data are similar to gaussian distribution, we will evaluate their Skewness an Kurtosis. From this quick view, we can observe that for classes `a` and `b` the covatiance matrix is similar, which is not the case with class `c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate skewness and kurtosis metrics for each \n",
    "# dimension of each class\n",
    "labels = ['A_x', 'A_y', 'B_x', 'B_y', 'C_x', 'C_y']\n",
    "\n",
    "kurtosis_metrics = stats.kurtosis(np.concatenate([a, b, c], axis = 1), axis = 0)\n",
    "\n",
    "skewness_metrics = stats.skew(np.concatenate([a, b, c], axis = 1), axis = 0)\n",
    "pd.DataFrame(index=labels, columns=['Kurtosis', 'Skewness'], data = np.asarray([kurtosis_metrics, skewness_metrics]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the values above, we have good approximations of gaussian values as all the kurtosis and skewness values are near to zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes rule:\n",
    "\n",
    "$ P(w_i|x)=\\frac{p(x|w_i)P(w_i)}{p(x)} $\n",
    "\n",
    "The discriminant function ($f$ monotonnally increasing) :\n",
    "\n",
    "$g_i(x)=f(P(w_i|x))$\n",
    "\n",
    "($f$ is a monotonnally increasing function)\n",
    "\n",
    "The decision surface is given by:\n",
    "\n",
    "$g_{ij}=g_i(x)-g_j(x)=0$\n",
    "\n",
    "For mean and covariance matrix:\n",
    "\n",
    "$\n",
    "u_x=\\cfrac{1}{n}\\sum_{j=1}^{n}{x}\n",
    "$\n",
    "\n",
    "$\n",
    "u_y=\\cfrac{1}{n}\\sum_{j=1}^{n}{y}\n",
    "$\n",
    "\n",
    "$\\Sigma ＝ \\cfrac{1}{n}(u-\\hat\\mu)^T(u-\\hat\\mu) \\quad$\n",
    "\n",
    "Since the 3 classes are modeled as Gaussians and have the same priori probabilities because they contain the same amount of data points, the discriminant function can be given as:\n",
    "\n",
    "$g_i(x)=-\\frac{1}{2}(x-\\mu_i)^T\\Sigma_i^{-1}(x-\\mu_i)+C$\n",
    "\n",
    "If 2 classes have the same covariance matrix (a and b in our case), the decision curve can be defined as:\n",
    "\n",
    "$g_i(x)=w_i^Tx+w_{i0}=(\\Sigma^{-1} \\mu_i)^Tx+ln(P(w_i)-\\frac{1}{2}\\mu_i^T \\Sigma^{-1}\\mu_i$\n",
    "\n",
    "In a more normal form, the decision curve can be defined as:\n",
    "\n",
    "$g_{ij}(x) = w^T(x - x_{0})$ with $w = \\Sigma^{-1}(\\mu_i - \\mu_j)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate paramters of Gaussian\n",
    "def get_coefficient(data):\n",
    "    temp_mean = data.mean(axis=0)\n",
    "\n",
    "    temp_len = len(data)\n",
    "    temp_sig = (np.dot((data-temp_mean).T, data-temp_mean)/temp_len)\n",
    "    # in the end we round the parameters to the closest integer\n",
    "    return temp_mean.round(), temp_sig.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and cov for classes\n",
    "means = []\n",
    "sigs = []\n",
    "i = 0\n",
    "for label, cl in zip(['a','b','c'], [a,b,c]):\n",
    "    mean, sig = get_coefficient(cl)\n",
    "    means.append(mean)\n",
    "    sigs.append(sig)\n",
    "    \n",
    "    print('mean for {} : {}'.format(label, mean))\n",
    "    print('covariance matrix for {} :\\n {}\\n'.format(label, sig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminant Hypotheses: We find that the covariance matrix $\\Sigma$ from classes `a` and`b` are identical. In theory, this means the quadratic term will disappear from the definition curve, the constant term will also be removed. So we end up with a linear function which is able to classify dataset `a` from dataset `b`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the coefficient of quadratic term, primary term and constant term \n",
    "# of x and y according to the equation above\n",
    "# in the end we round the coefficient to the closest integer\n",
    "def get_discriminant_curve_coeff(data_1, data_2):\n",
    "    mean_1, sig_1 = get_coefficient(data_1)\n",
    "    mean_2, sig_2 = get_coefficient(data_2)\n",
    "    \n",
    "    mean_11 , mean_12 = mean_1[0] , mean_1[1]\n",
    "    mean_21 , mean_22 = mean_2[0] , mean_2[1]\n",
    "    \n",
    "    sig_11 , sig_12 = sig_1[0][0] , sig_1[1][1]\n",
    "    sig_21 , sig_22 = sig_2[0][0] , sig_2[1][1]\n",
    "    \n",
    "    coef_x_2 = (sig_21-sig_11) * sig_12 * sig_22\n",
    "    coef_y_2 = (sig_22-sig_12) * sig_11*sig_21\n",
    "    \n",
    "    coef_x_1 = (-2 * mean_11 * sig_21 + 2 * mean_21 * sig_11) * sig_12 * sig_22\n",
    "    coef_y_1 = (-2 * mean_12 * sig_22 + 2 * mean_22 * sig_12) * sig_11 * sig_21\n",
    "    \n",
    "    coef_constant_1 =  mean_11 * mean_11 * sig_12 * sig_21 * sig_22 + mean_12 * mean_12 * sig_11 * sig_21 * sig_22\n",
    "    coef_constant_2 = -mean_21 * mean_21 * sig_11 * sig_12 * sig_22 - mean_22 * mean_22 * sig_11 * sig_12 * sig_21\n",
    "    \n",
    "    coef_constant = coef_constant_1 + coef_constant_2\n",
    "\n",
    "    # divide all coefficients by the smallest one\n",
    "    coeff_array = np.array([coef_x_2, coef_x_1, coef_y_2, coef_y_1, coef_constant])\n",
    "\n",
    "    coeff_array /= min(abs(coeff_array[np.flatnonzero(coeff_array)]))\n",
    "    \n",
    "    return np.round(coeff_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_curve(coeff, x_range, y_range):\n",
    "    return coeff[0]*x_range*x_range + coeff[1]*x_range + coeff[2]*y_range*y_range + coeff[3]*y_range + coeff[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_range = np.arange(-10, 10, 0.1)\n",
    "y_range = np.arange(-6, 6, 0.1)\n",
    "x_range, y_range = np.meshgrid(x_range, y_range)\n",
    "\n",
    "coeff_ab = get_discriminant_curve_coeff(a,b)\n",
    "coeff_ac = get_discriminant_curve_coeff(a,c)\n",
    "coeff_bc = get_discriminant_curve_coeff(b,c)\n",
    "\n",
    "curve_ab = get_curve(coeff_ab, x_range, y_range)\n",
    "curve_ac = get_curve(coeff_ac, x_range, y_range)\n",
    "curve_bc = get_curve(coeff_bc, x_range, y_range)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "for label_, cl, color_ in zip(['a', 'b', 'c'], [a, b, c], class_colors):\n",
    "    x, y = separateXY(cl)\n",
    "    ax.scatter(x, y, label=label_, color=color_)\n",
    "    \n",
    "for label_, con, color_ in zip(['AB', 'AC', 'BC'], [curve_ab, curve_ac, curve_bc], ['black','black','black']):\n",
    "    contour_item = ax.contour(x_range, y_range, con, 0, linestyles='dashed', colors=color_)\n",
    "    contour_item.levels = [label_ for val in contour_item.levels]\n",
    "    ax.clabel(contour_item, contour_item.levels, inline=True, fontsize=20, colors='black')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Mahalanobis distance\n",
    "For classes `a` and `b`, give the expression of the Mahalanobis distance used to classify a point in class `a` or `b`, and verify the obtained classification, in comparison with the \"complete\" Bayes classification, for a few points of the plane.\n",
    "\n",
    "According to the Bayesian classification for normal laws, if the covariance matrix $\\Sigma$ is not diagoal, the most probable class is the one which minimizes the Mahalanobis distance.\n",
    "\n",
    "Mahalanobis Distance:\n",
    "\n",
    "$d_m = \\left((x-\\mu_i)\\Sigma^{-1}(x-\\mu_i)\\right)^{1/2}$\n",
    "\n",
    "the class determined by Mahalanobis distance according to below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class=\n",
    "$\n",
    "\\left\\{\n",
    "\\begin{array}{rcl}\n",
    "a & & {d_a <= d_b}\\\\\n",
    "b & & {d_a > d_b}\n",
    "\\end{array} \\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mahalanobis distance of a data point from a dataset according to the equation above\n",
    "def get_mahalarnobis_distance(data, new_point):\n",
    "    mean , sig = get_coefficient(data)\n",
    "    inv_sig = np.linalg.inv(sig)\n",
    "    d_m = np.sqrt((new_point - mean).dot(inv_sig).dot((new_point - mean).T))\n",
    "    return d_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that reflect the performance of method of mahalarnobis distance\n",
    "# input: dataset and label\n",
    "# output: classification accuracy, list of data points that are correctly classified and list of data points that are wrongly classified\n",
    "def mahalarnobis_distance_performance(new_data, label):\n",
    "    total_num = len(new_data)\n",
    "    correct_num = 0\n",
    "    correct_list = []\n",
    "    wrong_list = []\n",
    "    \n",
    "    for data_ in new_data:\n",
    "        temp_dm_a = get_mahalarnobis_distance(a, data_)\n",
    "        temp_dm_b = get_mahalarnobis_distance(b, data_)\n",
    "        \n",
    "        # classification by minimizing the mahalanobis distance\n",
    "        prediction = np.argmin([temp_dm_a, temp_dm_b], axis = 0)\n",
    "        if prediction == label:\n",
    "            correct_num += 1\n",
    "            correct_list.append(data_)\n",
    "        else:\n",
    "            wrong_list.append(data_)\n",
    "            \n",
    "    accuracy=correct_num/total_num\n",
    "    \n",
    "    return accuracy, np.array(correct_list), np.array(wrong_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that reflect the performance of method of complete bayesian\n",
    "# input: dataset and label\n",
    "# output: classification accuracy, list of data points that are correctly classified and list of data points that are wrongly classified\n",
    "def complete_bayesian_performance(new_data, label):\n",
    "    total_num=len(new_data)\n",
    "    correct_num=0\n",
    "    correct_list=[]\n",
    "    wrong_list=[]\n",
    "    \n",
    "    for data_ in new_data:\n",
    "        temp_x = data_[0]\n",
    "        temp_y = data_[1]\n",
    "        prediction = get_curve(coeff_ab, temp_x, temp_y)>0\n",
    "        \n",
    "        if prediction == label:\n",
    "            correct_num+=1\n",
    "            correct_list+= [data_]\n",
    "        else:\n",
    "            wrong_list += [data_]\n",
    "    accuracy=correct_num/total_num\n",
    "    return accuracy, np.array(correct_list), np.array(wrong_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_a_mah, a_correct_list_mah, a_wrong_list_mah = mahalarnobis_distance_performance(a, 0)\n",
    "accuracy_b_mah, b_correct_list_mah, b_wrong_list_mah = mahalarnobis_distance_performance(b, 1)\n",
    "\n",
    "accuracy_a_bayes, a_correct_list_bayes, a_wrong_list_bayes = complete_bayesian_performance(a, 0)\n",
    "accuracy_b_bayes, b_correct_list_bayes, b_wrong_list_bayes = complete_bayesian_performance(b, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparaison of with Mahalanobis distance and Bayesian Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,6))\n",
    "ax[0].scatter(a_correct_list_mah[:, 0], a_correct_list_mah[:, 1], label = 'correctly classified a')\n",
    "ax[0].scatter(a_wrong_list_mah[:, 0], a_wrong_list_mah[:, 1], label = 'prediction:b,  ground truth:a')\n",
    "ax[0].scatter(b_correct_list_mah[:, 0], b_correct_list_mah[:, 1], label = 'correctly classified b')\n",
    "ax[0].scatter(b_wrong_list_mah[:, 0], b_wrong_list_mah[:, 1], label = 'prediction:a,  ground truth:b')\n",
    "ax[0].set_title(\"Classification Performance by Mahalanobis Distance\")\n",
    "\n",
    "ax[1].scatter(a_correct_list_bayes[:, 0], a_correct_list_bayes[:, 1], label = 'correctly classified a')\n",
    "ax[1].scatter(a_wrong_list_bayes[:, 0], a_wrong_list_bayes[:, 1], label = 'prediction:b,  ground truth:a')\n",
    "ax[1].scatter(b_correct_list_bayes[:, 0], b_correct_list_bayes[:, 1], label = 'correctly classified b')\n",
    "ax[1].scatter(b_wrong_list_bayes[:, 0], b_wrong_list_bayes[:, 1], label = 'prediction:a,  ground truth:b')\n",
    "contour_item = ax[1].contour(x_range, y_range, curve_ab, 0,linestyles='dotted', colors='black')\n",
    "contour_item.levels = [' AB Separation' for val in contour_item.levels]\n",
    "ax[1].clabel(contour_item, contour_item.levels, inline=True, fontsize=10, colors='black')\n",
    "ax[1].set_title(\"Classification Performance by Bayes Method\")\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('By Bayes Method')\n",
    "print('Accuracy for a data set = {}'.format(accuracy_a_bayes))\n",
    "print('Accuracy for b data set = {}'.format(accuracy_b_bayes))\n",
    "print('Accuracy for all data in a and b data sets = {:2f}'.format(0.5 * (accuracy_a_bayes + accuracy_b_bayes)))\n",
    "print('-----------------------------------------------------------------')\n",
    "print('By Mahalanobis Distance')\n",
    "print('Accuracy for a data set = {}'.format(accuracy_a_mah))\n",
    "print('Accuracy for b data set = {}'.format(accuracy_b_mah))\n",
    "print('Accuracy for all data in a and b data sets = {:2f}'.format(0.5 * (accuracy_a_mah + accuracy_b_mah)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the Bayasian classification for normal laws, if the covariance matrix $\\Sigma$ is not diagoal, the most probable class is the one which minimizes the Mahalanobis distance.\n",
    "\n",
    "The plots of classification and numerical accuracy show that mahalanobis distance and complete Bayesian classifier share the same result and performance in this case, which proves the theoretical hypothesis, along with the same prior probabilities of a and b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "In this part, we aim to classify digits using the complete version of MNIST digits dataset. The dataset consists of 60'000 training images and 10'000 test images of handwritten digits. Each image has size 28x28, and has assigned a label from zero to nine, denoting the digits value. Given this data, your task is to construct a Multilayer Perceptron (MLP) by Tensorflow framework and evaluate it on MNIST the test images.\n",
    "\n",
    "To facilitate your task, we will walk you through the required steps by a simple tutorial. By finishing this part, you learn the basic workflow of using TensorFlow with a simple linear model and multilayer perceptron (MLP) model, respectively.\n",
    "\n",
    "As the starting point, we aim to work with MNIST dataset. After loading the dataset, we define and optimize a simple mathematical model in TensorFlow. The results are then plotted and discussed. First, we import tensorflow with other needed packages. If Tensorflow is not installed on your computer, you can install it by running `pip install --upgrade tensorflow` command in your terminal, after activating your enviroment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##ATTENTION CHANGES\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "##END ATTENTION CHANGES\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can figure out your tensorflow version by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the MNIST dataset (all 4 files) from http://yann.lecun.com/exdb/mnist/ under lab-03-data/part2. You can then use the script provided below to extract and load training and testing images in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filename, image_shape, image_number):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(np.prod(image_shape) * image_number)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = data.reshape(image_number, image_shape[0], image_shape[1])\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_labels(filename, image_number):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * image_number)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the openend files\n",
    "image_shape = (28, 28)\n",
    "train_set_size = 60000\n",
    "test_set_size = 10000\n",
    "\n",
    "data_part2_folder = os.path.join(data_base_path, data_folder, 'part2')\n",
    "\n",
    "train_images_path = os.path.join(data_part2_folder, 'train-images-idx3-ubyte.gz')\n",
    "train_labels_path = os.path.join(data_part2_folder, 'train-labels-idx1-ubyte.gz')\n",
    "test_images_path = os.path.join(data_part2_folder, 't10k-images-idx3-ubyte.gz')\n",
    "test_labels_path = os.path.join(data_part2_folder, 't10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "train_images_extract = extract_data(train_images_path, image_shape, train_set_size)\n",
    "test_images_extract = extract_data(test_images_path, image_shape, test_set_size)\n",
    "train_labels_extract = extract_labels(train_labels_path, train_set_size)\n",
    "test_labels_extract = extract_labels(test_labels_path, test_set_size)\n",
    "\n",
    "print(\"Training Set \", train_images_extract.shape, train_labels_extract.shape)\n",
    "print(\"Test Set\", test_images_extract.shape, test_labels_extract.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset has now been loaded and consists of images and associated labels (i.e. classifications of the images). The dataset is split into 2 mutually exclusive sub-sets (60000 training images and 10000 test images, respectively). You can display the images in each sub-set. They should match the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function for plotting images\n",
    "\n",
    "We define a function to plot 9 images in a 3x3 grid, and writing the true and predicted classes below each image. Then we plot a few images with their labels, to see if data is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "\n",
    "def plot_images(images, cls_true, cls_pred = None):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(image_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()\n",
    "\n",
    "# Get the first images from the test-set.\n",
    "images = test_images_extract[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = test_labels_extract[0:9]\n",
    "\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true = cls_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding and Image Flattened Mode\n",
    "\n",
    "Our labels contain a list of predictions for our examples, e.g. [1, 9, ...]. In tensorflow, the label has to be converted to the encoding format as so-called One-Hot encoding. This means the labels have been converted from a single number to a vector whose length equals the number of possible classes. All elements of the vector are zero except for the $i$'th element which is one and means the class is $i$.\n",
    "\n",
    "In addition, because we are going to use linear layers and multiplications of the neural network, you always want your data to be a (1 or) 2-dimensional matrix, where each row is the vector representing your data. Therefore, it would be more complicated and less efficient without reshaping images first. Here, we need to flatten images before passing them to our model. As an example, please print out the One-Hot encoded labels for the first 5 images in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat(dataset, labels):\n",
    "    \"\"\"\n",
    "        Reformat the data to the one-hot and flattened mode\n",
    "    \"\"\"\n",
    "    n_dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "\n",
    "    # Convert to the one hot format\n",
    "    n_labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "\n",
    "    return n_dataset, n_labels\n",
    "\n",
    "\n",
    "num_labels = 10\n",
    "\n",
    "train_images, train_labels = reformat(train_images_extract, train_labels_extract)\n",
    "test_images, test_labels = reformat(test_images_extract, test_labels_extract)\n",
    "\n",
    "# Display the files\n",
    "print(\"Training Set - shape: \", train_images.shape, train_labels.shape)\n",
    "print(\"Test Set - shape: \", test_images.shape, test_labels.shape)\n",
    "print(\"Test Set labels - [1 to 5]: \\n\", test_labels[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Graph\n",
    "\n",
    "The entire purpose of TensorFlow is to have a so-called computational graph that can be executed much more efficiently than if the same calculations were to be performed directly in Python. TensorFlow can automatically calculate the gradients that are needed to optimize the variables of the graph to make the model perform better. This is because the graph is a combination of simple mathematical expressions so the gradient of the entire graph can be calculated using the chain rule for derivatives.\n",
    "\n",
    "\n",
    "A TensorFlow graph consists of the following parts which will be detailed below:\n",
    "\n",
    "* Placeholder variables used to change the input to the graph.\n",
    "* Model variables that are going to be optimised to make the model perform better.\n",
    "* The model which is essentially just a mathematical function that calculates some output given the input in the placeholder variables and the model variables.\n",
    "* A cost measure that can be used to guide the optimization of the variables.\n",
    "* An optimization method which updates the variables of the model.\n",
    "\n",
    "In addition, the TensorFlow graph may also contain various debugging statements, e.g. for logging data to be displayed using TensorBoard, which is not covered here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default graph\n",
    "\n",
    "As the starting point, we create a new computational graph via the `tf.Graph` constructor. To add operations to this graph, we must register it as the default graph. The way the TensorFlow API is designed, library routines that create new operation nodes always attach these to the current default graph. We register our graph as the default by using it as a Python context manager in a `with-as` statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder variables\n",
    "\n",
    "Placeholder variables serve as the input to the graph that we may change each time we execute the graph. We call this feeding the placeholder variables and it is demonstrated further below.\n",
    "\n",
    "First, we define the placeholder variable for the input images including train and validation set. This allows us to change the images that are input to the TensorFlow graph. This is a so-called tensor, which just means that it is a multi-dimensional vector or matrix. The data-type is set to `float32` and the shape is set to `[None, image_size_flat]`, where `None` means that the tensor may hold an arbitrary number of images with each image being reshaped into a vector of length `image_size_flat` which is equal to image_size * image_size. In addition, we create a constant tensor for the validation and test set since they are fixed and will be evaluated later.   \n",
    "\n",
    "Next we have the placeholder variable for the true labels associated with the images that were input in the placeholder variable `TF_TRAIN_DATASET`. The shape of this placeholder variable is `[None, num_labels]` which means it may hold an arbitrary number of labels and each label is a vector of length `num_labels` which is 10 in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables to be optimized\n",
    "\n",
    "Apart from the placeholder variables that were defined above and which serve as feeding input data into the model, there are also some model variables that must be changed by TensorFlow to make the model perform better on the training data.\n",
    "\n",
    "In fact, the model variables are the network hidden layer parameters including layer's weights and biases. The first variable that must be optimized are `WEIGHTS` and defined here as a TensorFlow variable that must be initialized and whose shape is `[image_size_flat, num_labels]` for the simple linear model, so it is a 2-dimensional tensor (or matrix) with `image_size_flat` rows and `num_labels` columns. There are various ways to initialize the weights like initialization with zeros `tf.zeros`, `tf.truncated_normal` and `tf.random_normal`, which output random values from a normal distribution and you can specify the mean and The standard deviation (`stddev`) of the normal distribution.\n",
    "\n",
    "The second variable that must be optimized is called `BIASES` and is defined as a 1-dimensional tensor (or vector) of length `num_labels`.\n",
    "\n",
    "Last but not least, you should name every important operation in your code. In complex models, it is good practice to use `scopes`. The important point is that if you want to later use some operation, you have to either name it or put it into a collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "This simple mathematical model multiplies the training images in the placeholder variable `TF_TRAIN_DATASET` with the `WEIGHTS` and then adds the ` BIASES`. The result is a matrix of shape `[num_images, num_labels]` because `TF_TRAIN_DATASET` has shape `[num_images, image_size_flat]` and `WEIGHTS` has shape `[image_size_flat, num_labels]`, so the multiplication of those two matrices is a matrix with shape `[num_images, num_labels]` and then the `BIASES` vector is added to each row of that matrix.\n",
    "\n",
    "However, these estimates are a bit rough and difficult to interpret because the numbers may be very small or large, so we want to normalize them so that each row of the `LOGITS` matrices sums to one (see below code), and each element is limited between zero and one. This is calculated using the so-called softmax function `tf.nn.softmax`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost-function to be optimized\n",
    "\n",
    "To make the model better at classifying the input images, we must somehow change the variables for `WEIGHTS` and `BIASES`. To do this, we first need to know how well the model currently performs by comparing the predicted output of the model to the desired output.\n",
    "\n",
    "The cross-entropy is a performance measure used in classification. The cross-entropy is a continuous function that is always positive and if the predicted output of the model exactly matches the desired output then the cross-entropy equals zero. The goal of optimization is, therefore, to minimise the cross-entropy, so it gets as close to zero as possible by changing the `WEIGHTS` and `BIASES` of the model.\n",
    "\n",
    "TensorFlow has a built-in function for calculating the cross-entropy using `tf.nn.softmax_cross_entropy_with_logits`. Note that it uses the values of the `LOGITS` in train, validation and test sets because it also calculates the softmax internally. In order to use the cross-entropy to guide the optimization of the model's variables we need a single scalar value, so we simply take the average of the cross-entropy using (`tf.reduce_mean`) for all the image classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization method\n",
    "\n",
    "Now that we have a cost measure that must be minimized, we can then create an optimizer. In this case it is the basic form of Gradient Descent where the step-size is set to 0.5.\n",
    "\n",
    "Note that optimization is not performed at this point. In fact, nothing is calculated at all, we just add the optimizer-object to the TensorFlow graph for later execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size_flat = image_size * image_size\n",
    "\n",
    "# Create a new graph\n",
    "GRAPH = tf.Graph()\n",
    "\n",
    "# Register the graph as the default one to add nodes\n",
    "with GRAPH.as_default():\n",
    "    \n",
    "    # Define placeholders\n",
    "    TF_TRAIN_DATASET = tf.placeholder(tf.float32, shape = (None, image_size * image_size))\n",
    "    TF_TRAIN_LABELS = tf.placeholder(tf.float32, shape = (None, num_labels))\n",
    "    TF_TEST_DATASET = tf.constant(test_images)\n",
    "    \n",
    "    with tf.name_scope(\"Linear_model\"):\n",
    "        \n",
    "        \"\"\"\n",
    "           Initialize weights and biases\n",
    "        \"\"\"\n",
    "        \n",
    "        WEIGHTS = tf.Variable(tf.random_normal(shape=[image_size_flat, num_labels], stddev=0.1))\n",
    "        BIASES = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "        \"\"\"\n",
    "           Compute the logits WX + b\n",
    "        \"\"\" \n",
    "        TRAIN_LOGITS = tf.matmul(TF_TRAIN_DATASET, WEIGHTS) + BIASES\n",
    "        TEST_LOGITS = tf.matmul(TF_TEST_DATASET, WEIGHTS) + BIASES\n",
    "    \n",
    "        \"\"\"\n",
    "           Softmax function\n",
    "        \"\"\"   \n",
    "        TRAIN_PREDICTION = tf.nn.softmax(TRAIN_LOGITS)\n",
    "        TEST_PREDICTION = tf.nn.softmax(TEST_LOGITS)\n",
    "    \n",
    "        \"\"\"\n",
    "           Cost-function\n",
    "        \"\"\"\n",
    "        CROSS_ENTROPY = tf.nn.softmax_cross_entropy_with_logits_v2(logits = TRAIN_LOGITS, labels = TF_TRAIN_LABELS)\n",
    "        COST = tf.reduce_mean(CROSS_ENTROPY)\n",
    "        # Optimizer\n",
    "        OPTIMIZER = tf.train.GradientDescentOptimizer(learning_rate = 0.5).minimize(COST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-functions to compute the accuracy\n",
    "\n",
    "Now, we need a performance measure to display the progress to the user. This can be done via defining a function to compute accuracy to see whether the predicted class equals the true class of each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    \"\"\"\n",
    "        Divides the number of true predictions to the number of total predictions\n",
    "    \"\"\"\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TensorFlow session and RUN!\n",
    "\n",
    "Once the TensorFlow graph has been created, we have to create a TensorFlow session which is used to execute the graph and train our model. For this, we enter a session environment using a `tf.Session` as a context manager. We pass our graph  object to its constructor, so that it knows which graph to manage. To then execute nodes, we have several options. The   most general way is to call Session.run() and pass a list of tensors we wish to compute. Alternatively, we may call `eval()` on  tensors and `run()` on  operations directly.\n",
    "\n",
    "Before evaluating any other node, we must first ensure that the variables in our graph are initialized. Theoretically,  we could `run` the `Variable.initializer` operation for each variable. However, one most often just uses the\n",
    "`tf.initialize_all_variables()` utility operation provided by TensorFlow, which in turn executes the `initializer` \n",
    "operation for each `Variable` in the graph. Then, we can perform a certain number of iterations of stochastic gradient  descent, fetching an example and label from the MNIST dataset each time and feeding it to the run routine. \n",
    "\n",
    "One important point is that, there are many images in the training-set. Therefore, it takes a long time to calculate the gradient of the model using all these images. We therefore use Stochastic Gradient Descent which only uses a small batch of images in each iteration of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_iterations = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.Session(graph = GRAPH) as session:\n",
    "    \"\"\"\n",
    "        Start the above variable initialization\n",
    "    \"\"\"\n",
    "    tf.global_variables_initializer().run()\n",
    "    # v1: tf.initialize_all_variables().run()\n",
    "\n",
    "    for step in range(num_iterations):\n",
    "        \"\"\"\n",
    "            Generate a random base and then generate a minibatch\n",
    "        \"\"\"\n",
    "        BASE = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        BATCH_DATA = train_images[BASE:(BASE + batch_size), :]\n",
    "        BATCH_LABELS = train_labels[BASE:(BASE + batch_size), :]\n",
    "        \"\"\"\n",
    "            Feed the current session with batch data\n",
    "        \"\"\"\n",
    "        FEED_DICT = {TF_TRAIN_DATASET:BATCH_DATA, TF_TRAIN_LABELS:BATCH_LABELS}\n",
    "        _, l, predictions = session.run([OPTIMIZER, COST, TRAIN_PREDICTION], feed_dict = FEED_DICT)\n",
    "\n",
    "        if(step % 500 == 0):\n",
    "            print(\"Step {}:\\nLoss: {:.02f}\\nTrain accuracy: {:.02f} %\\nTest accuracy: {:.02f} %\"\\\n",
    "                  .format(step, l, accuracy(predictions, BATCH_LABELS), accuracy(TEST_PREDICTION.eval(), test_labels)))\n",
    "            print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron (MLP)\n",
    "\n",
    "The next architecture we are going to work with is multilayer perceptron (MLP). An MLP can be viewed as a logistic regression classifier where the input is first transformed using some non-linear transformations of the intermediate network layers. These intermediate layers are referred to as a `hidden layers`. Here, we are going to define TF graph for the MLP model with two hidden layers and one output layer. \n",
    "\n",
    "In addition, another milestone in the deep learning revolution, e.g. the techniques that now permit the routine development of very deep neural networks is adoptation of a rectified linear activation unit (Relu) that uses the rectifier function for the hidden layers. In tensorflow we use `tf.nn.relu` for Relu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "In our example, the MLP model has two hidden layers with sizes of `hidden_nodes_1=1024` and `hidden_nodes_2=512`, respectively. In MLP, every element of a previous layer is connected to every element of the next layer. For example, the weights in the second hidden layer has `shape=[hidden_nodes_1, hidden_nodes_2]`. \n",
    "\n",
    "Now, lets define MLP graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the number of nodes for the hidden layers\n",
    "hidden_nodes_1=1024\n",
    "hidden_nodes_2=512\n",
    "\n",
    "MLP_GRAPH = tf.Graph()\n",
    "\n",
    "with MLP_GRAPH.as_default():\n",
    "    \"\"\"\n",
    "        For the training data we use place holders in order to feed them\n",
    "        in the run time with those mini bitches :D\n",
    "    \"\"\"\n",
    "    TF_TRAIN_DATASET = tf.placeholder(tf.float32, shape=(None, image_size * image_size))\n",
    "    TF_TRAIN_LABELS = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "    TF_TEST_DATASET = tf.constant(test_images)\n",
    "\n",
    "    \"\"\"\n",
    "       The first hidden layer with 1024 nodes\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.name_scope(\"FirstHidden\"):\n",
    "        \"\"\"\n",
    "            Initialize the hidden weights and biases\n",
    "        \"\"\"\n",
    "        HIDDEN_WEIGHTS_1=tf.Variable(tf.random_normal(shape=[image_size * image_size, hidden_nodes_1], stddev=0.1))\n",
    "        HIDDEN_BIASES_1 =tf.Variable(tf.random_normal(shape=[hidden_nodes_1], stddev=0.1))\n",
    "\n",
    "        \"\"\"\n",
    "            Compute the logits WX + b and then apply D(S(WX + b), L) on them for the hidden layer\n",
    "            The relu is applied on the hidden layer nodes only\n",
    "        \"\"\"\n",
    "        TRAIN_HIDDEN_LOGITS_1 = tf.nn.relu(tf.matmul(TF_TRAIN_DATASET, HIDDEN_WEIGHTS_1) + HIDDEN_BIASES_1)\n",
    "        TEST_HIDDEN_LOGITS_1 = tf.nn.relu(tf.matmul(TF_TEST_DATASET, HIDDEN_WEIGHTS_1) + HIDDEN_BIASES_1)\n",
    "        \n",
    "    \"\"\"\n",
    "       The second hidden layer with 512 nodes\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.name_scope(\"SecondHidden\"):\n",
    "        \n",
    "        #Add your code for implementation of the second layer here\n",
    "        # Initialize variables of second hidden layer\n",
    "        HIDDEN_WEIGHTS_2 = tf.Variable(tf.random_normal(shape = [hidden_nodes_1, hidden_nodes_2],\n",
    "                                                        stddev = 0.1))\n",
    "        HIDDEN_BIASES_2 = tf.Variable(tf.random_normal(shape = [hidden_nodes_2], stddev = 0.1))\n",
    "\n",
    "        # Compute output of second hidden layer\n",
    "        TRAIN_HIDDEN_LOGITS_2 = tf.nn.relu(tf.matmul(TRAIN_HIDDEN_LOGITS_1, HIDDEN_WEIGHTS_2) + HIDDEN_BIASES_2)\n",
    "        TEST_HIDDEN_LOGITS_2 = tf.nn.relu(tf.matmul(TEST_HIDDEN_LOGITS_1, HIDDEN_WEIGHTS_2) + HIDDEN_BIASES_2)\n",
    "\n",
    "\n",
    "    with tf.name_scope(\"Softmax-Linear\"):\n",
    "        \"\"\"\n",
    "            Initialize the main weights and biases\n",
    "        \"\"\"\n",
    "        WEIGHTS=tf.Variable(tf.random_normal(shape=[hidden_nodes_2, num_labels], stddev=0.1))\n",
    "        BIASES=tf.Variable(tf.random_normal(shape=[num_labels], stddev=0.1))\n",
    "\n",
    "        \"\"\"\n",
    "            Compute the logits WX + b and the apply D(S(WX + b), L) on them for the final layer\n",
    "        \"\"\"\n",
    "        TRAIN_LOGITS = tf.matmul(TRAIN_HIDDEN_LOGITS_2, WEIGHTS) + BIASES\n",
    "        TEST_LOGITS = tf.matmul(TEST_HIDDEN_LOGITS_2, WEIGHTS) + BIASES\n",
    "\n",
    "        LOSS = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=TRAIN_LOGITS, labels=TF_TRAIN_LABELS))\n",
    "        # v1: LOSS = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=TRAIN_LOGITS, labels=TF_TRAIN_LABELS))\n",
    "\n",
    "        OPTIMIZER = tf.train.GradientDescentOptimizer(0.0005).minimize(LOSS)\n",
    "        tf.add_to_collection(\"activation\", TRAIN_LOGITS)\n",
    "\n",
    "        TRAIN_PREDICTION = tf.nn.softmax(TRAIN_LOGITS)\n",
    "        TEST_PREDICTION = tf.nn.softmax(TEST_LOGITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function to perform optimization iterations\n",
    "\n",
    "Here, we build a function for performing a number of optimization iterations so as to gradually improve the weights and biases of the MLP model. In each iteration, a new batch of data is selected from the training-set and then TensorFlow executes the optimizer using those training samples. The inputs of the function are the number of iterations `num_iterations` and number of used samples `num_samples` to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(num_iterations, num_samples):\n",
    "    \n",
    "    with tf.Session(graph = MLP_GRAPH) as session:\n",
    "        \"\"\"\n",
    "            Start the above variable initialization\n",
    "        \"\"\"\n",
    "        tf.global_variables_initializer().run()\n",
    "        # v1: tf.initialize_all_variables().run()\n",
    "\n",
    "        for step in range(num_iterations):\n",
    "            \"\"\"\n",
    "                Select the desired samples\n",
    "            \"\"\"\n",
    "            TRAIN_DATASET_S = train_images[:num_samples]\n",
    "            TRAIN_LABELS_S = train_labels[:num_samples]\n",
    "            \"\"\"\n",
    "                Generate a random base and then generate a minibatch\n",
    "            \"\"\"\n",
    "            \n",
    "            indices = np.random.choice(range(TRAIN_LABELS_S.shape[0]\n",
    "                                             ), batch_size)\n",
    "            BATCH_DATA = TRAIN_DATASET_S[indices, :]\n",
    "            BATCH_LABELS = TRAIN_LABELS_S[indices, :]\n",
    "\n",
    "            \"\"\"\n",
    "                Feed the current session with batch data\n",
    "            \"\"\"\n",
    "            FEED_DICT = {TF_TRAIN_DATASET: BATCH_DATA, TF_TRAIN_LABELS: BATCH_LABELS}\n",
    "            _, l, predictions = session.run([OPTIMIZER, LOSS, TRAIN_PREDICTION], feed_dict=FEED_DICT)\n",
    "\n",
    "        acc=accuracy(TEST_PREDICTION.eval(), test_labels)\n",
    "        print(\"Test accuracy: \", accuracy(TEST_PREDICTION.eval(), test_labels))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "Now, we want to train and compare the MLP model with two hidden-layers that we defined before using different number of training samples. You should use varying number of training samples including 5000, 10000 and 20000 images and report the MLP accuracy for each experiment. Please set `num_iterations` to 2000.\n",
    "\n",
    "Hint: You can use `optimize` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SIZES = [5000, 10000, 20000]\n",
    "\n",
    "#Add your code here\n",
    "num_iterations = 2000\n",
    "for i in TRAINING_SIZES:\n",
    "    print('Accuracy of Training Size of {} : '.format(i))\n",
    "    accuracy_results = optimize(num_iterations, i)\n",
    "print('------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
