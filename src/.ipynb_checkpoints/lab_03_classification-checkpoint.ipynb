{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IAPR 2020:][iapr2020] Lab 3 ‒  Classification\n",
    "\n",
    "**Authors:** Sven Borden, Sorya Jullien, Artur Jesslen  \n",
    "**Due date:** 03.05.2020\n",
    "\n",
    "[iapr2020]: https://github.com/LTS5/iapr-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before running this notebook, verify that you have all requirements by running this command in your terminal:\n",
    "`python -m pip install numpy pandas scipy sympy tensorflow matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "In this part, we will study classification based on the data available in the Matlab file `classification.mat` that you will under `lab-03-data/part1`.\n",
    "There are 3 data sets in this file, each one being a training set for a given class.\n",
    "They are contained in variables `a`, `b` and `c`.\n",
    "\n",
    "**Note**: we can load Matlab files using the [scipy.io] module.\n",
    "\n",
    "[scipy.io]: https://docs.scipy.org/doc/scipy/reference/io.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract relevant data\n",
    "We first need to extract the `lab-03-data.tar.gz` archive.\n",
    "To this end, we use the [tarfile] module from the Python standard library.\n",
    "\n",
    "[tarfile]: https://docs.python.org/3.6/library/tarfile.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import scipy.io\n",
    "from scipy.stats import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Sympy modules\n",
    "from sympy import symbols\n",
    "from sympy import plot_implicit\n",
    "from sympy import Eq\n",
    "from sympy.solvers import solve\n",
    "from mpmath import log\n",
    "from sympy import Matrix\n",
    "\n",
    "# defining the files path\n",
    "import matplotlib.pyplot as plt\n",
    "data_base_path = os.path.join(os.pardir, 'data')\n",
    "data_folder = 'lab-03-data'\n",
    "tar_path = os.path.join(data_base_path, data_folder + '.tar.gz')\n",
    "with tarfile.open(tar_path, mode='r:gz') as tar:\n",
    "    tar.extractall(path=data_base_path)\n",
    "    \n",
    "data_part1_path = os.path.join(data_base_path, data_folder, 'part1', 'classification.mat')\n",
    "matfile = scipy.io.loadmat(data_part1_path)\n",
    "a = matfile['a']\n",
    "b = matfile['b']\n",
    "c = matfile['c']\n",
    "\n",
    "print(a.shape, b.shape, c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Bayes method\n",
    "Using the Bayes method, give the analytical expression of the separation curves between those three classes.\n",
    "Do reasonable hypotheses about the distributions of those classes and estimate the corresponding parameters based on the given training sets.\n",
    "Draw those curves on a plot, together with the training data.\n",
    "For simplicity reasons, round the estimated parameters to the closest integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_colors = ['red', 'green', 'blue'] #used later for plotting colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot more easily n by 2 matrix (n vectors)\n",
    "def separateXY(matrix):\n",
    "    return matrix[:,0], matrix[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot raw classes\n",
    "plt.figure()\n",
    "for label_, cl, color_ in zip(['a','b','c'],[a, b, c], class_colors):\n",
    "    x, y = separateXY(cl)\n",
    "    plt.scatter(x, y, label=label_, color=color_)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we can tell that the distributions for each class is reasonably gaussian. We also can see than each class is balanced in the number of samples. To quantify how much data are similar to gaussian distribution, we will evaluate their Skewness an Kurtosis. From this quick view, we can observe that for classes `a` and `b` the covariance matrix is similar, which is not the case with class `c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate skewness and kurtosis metrics for each dimension of each class\n",
    "labels = ['A_x', 'A_y', 'B_x', 'B_y', 'C_x', 'C_y']\n",
    "\n",
    "kurtosis_metrics = stats.kurtosis(np.concatenate([a, b, c], axis = 1), axis = 0)\n",
    "\n",
    "skewness_metrics = stats.skew(np.concatenate([a, b, c], axis = 1), axis = 0)\n",
    "pd.DataFrame(index=labels, columns=['Kurtosis', 'Skewness'], data = np.asarray([kurtosis_metrics, skewness_metrics]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the values above, we have good approximations of gaussian values as all the kurtosis and skewness values are near to zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes rule for a class defined by the points $X = [x,y]$ :\n",
    "\n",
    "$ P(w_i|X)=\\frac{p(X|w_i)P(w_i)}{p(X)} $\n",
    "\n",
    "The discriminant function ($f$ a monotonnally increasing function) :\n",
    "\n",
    "$g_i(X)=f(P(w_i|X))$\n",
    "\n",
    "The decision surface is given by:\n",
    "\n",
    "$g_{ij}=g_i(X)-g_j(X)=0$\n",
    "\n",
    "For mean and covariance matrix:\n",
    "\n",
    "$\n",
    "\\mu_x=\\cfrac{1}{n}\\sum_{j=1}^{n}{x}\n",
    "$\n",
    "\n",
    "$\n",
    "\\mu_y=\\cfrac{1}{n}\\sum_{j=1}^{n}{y}\n",
    "$\n",
    "\n",
    "$\\Sigma ＝ \\cfrac{1}{n}(X-\\hat\\mu)^T(X-\\hat\\mu) \\quad$\n",
    "\n",
    "Since the 3 classes are modeled as Gaussians and have the same priori probabilities because they contain the same amount of data points, the discriminant function can be given as:\n",
    "\n",
    "$g_i(X)=-\\frac{1}{2}(X-\\mu_i)^T\\Sigma_i^{-1}(X-\\mu_i)+C$\n",
    "\n",
    "If 2 classes have the same covariance matrix (a and b in our case), the decision curve can be defined as:\n",
    "\n",
    "$g_i(X)=w_i^TX+w_{i0}=(\\Sigma^{-1} \\mu_i)^TX+ln(P(w_i)-\\frac{1}{2}\\mu_i^T \\Sigma^{-1}\\mu_i$\n",
    "\n",
    "In a more normal form, the decision curve can be defined as:\n",
    "\n",
    "$g_{ij}(X) = w^T(X - X_{0})$ with $w = \\Sigma^{-1}(\\mu_i - \\mu_j)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate parameters of Gaussian\n",
    "def get_coefficient(data):\n",
    "    #mean\n",
    "    temp_mean = data.mean(axis=0)\n",
    "\n",
    "    #covariance\n",
    "    temp_len = len(data)\n",
    "    temp_sig = (np.dot((data-temp_mean).T, data-temp_mean)/temp_len)\n",
    "    \n",
    "    # in the end we round the parameters to the closest integer\n",
    "    return temp_mean.round(), temp_sig.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and covariance for each class\n",
    "means = []\n",
    "sigs = []\n",
    "i = 0\n",
    "for label, cl in zip(['a','b','c'], [a,b,c]):\n",
    "    mean, sig = get_coefficient(cl)\n",
    "    means.append(mean)\n",
    "    sigs.append(sig)\n",
    "    \n",
    "    print('mean for {} : {}'.format(label, mean))\n",
    "    print('covariance matrix for {} :\\n {}\\n'.format(label, sig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminant Hypotheses: We find that the covariance matrix $\\Sigma$ from classes `a` and`b` are identical. In theory, this means the quadratic term will disappear from the definition curve, the constant term will also be removed. So we end up with a linear function which is able to classify dataset `a` from dataset `b`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the coefficient of quadratic terms, primary terms and constant term \n",
    "# of x and y according to the equation above\n",
    "# in the end we round the coefficient to the closest integer\n",
    "def get_discriminant_curve_coeff(data_1, data_2):\n",
    "    # mean and covariance for each data\n",
    "    mean_1, sig_1 = get_coefficient(data_1)\n",
    "    mean_2, sig_2 = get_coefficient(data_2)\n",
    "    \n",
    "    # separation of mean and covariance values along the x and y axis\n",
    "    mean_11 , mean_12 = mean_1[0] , mean_1[1]\n",
    "    mean_21 , mean_22 = mean_2[0] , mean_2[1]\n",
    "    \n",
    "    sig_11 , sig_12 = sig_1[0][0] , sig_1[1][1]\n",
    "    sig_21 , sig_22 = sig_2[0][0] , sig_2[1][1]\n",
    "    \n",
    "    # coefficient of quadratic terms x^2, y^2\n",
    "    coef_x_2 = (sig_21-sig_11) * sig_12 * sig_22\n",
    "    coef_y_2 = (sig_22-sig_12) * sig_11*sig_21\n",
    "    \n",
    "    # coefficient of primary terms x, y\n",
    "    coef_x_1 = (-2 * mean_11 * sig_21 + 2 * mean_21 * sig_11) * sig_12 * sig_22\n",
    "    coef_y_1 = (-2 * mean_12 * sig_22 + 2 * mean_22 * sig_12) * sig_11 * sig_21\n",
    "    \n",
    "    # coefficient of constant term\n",
    "    coef_constant_1 =  mean_11 * mean_11 * sig_12 * sig_21 * sig_22 + mean_12 * mean_12 * sig_11 * sig_21 * sig_22\n",
    "    coef_constant_2 = -mean_21 * mean_21 * sig_11 * sig_12 * sig_22 - mean_22 * mean_22 * sig_11 * sig_12 * sig_21\n",
    "    \n",
    "    coef_constant = coef_constant_1 + coef_constant_2\n",
    "\n",
    "    # divide all coefficients by the smallest one to have small coefficient values while keeping the same ratio\n",
    "    coeff_array = np.array([coef_x_2, coef_x_1, coef_y_2, coef_y_1, coef_constant])\n",
    "\n",
    "    coeff_array /= min(abs(coeff_array[np.flatnonzero(coeff_array)]))\n",
    "    \n",
    "    return np.round(coeff_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_curve(coeff, x_range, y_range):\n",
    "    return coeff[0]*x_range*x_range + coeff[1]*x_range + coeff[2]*y_range*y_range + coeff[3]*y_range + coeff[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# axis plot initialization\n",
    "x_range = np.arange(-10, 10, 0.1)\n",
    "y_range = np.arange(-6, 6, 0.1)\n",
    "x_range, y_range = np.meshgrid(x_range, y_range)\n",
    "\n",
    "# compute the coefficients of the discriminant curves between 2 classes\n",
    "coeff_ab = get_discriminant_curve_coeff(a,b)\n",
    "coeff_ac = get_discriminant_curve_coeff(a,c)\n",
    "coeff_bc = get_discriminant_curve_coeff(b,c)\n",
    "\n",
    "# compute the curve between 2 classes using the coefficients computed above\n",
    "curve_ab = get_curve(coeff_ab, x_range, y_range)\n",
    "curve_ac = get_curve(coeff_ac, x_range, y_range)\n",
    "curve_bc = get_curve(coeff_bc, x_range, y_range)\n",
    "\n",
    "# plot the classes \n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "for label_, cl, color_ in zip(['a', 'b', 'c'], [a, b, c], class_colors):\n",
    "    x, y = separateXY(cl)\n",
    "    ax.scatter(x, y, label=label_, color=color_)\n",
    "    \n",
    "# plot the classification curves\n",
    "for label_, con, color_ in zip(['AB', 'AC', 'BC'], [curve_ab, curve_ac, curve_bc], ['black','black','black']):\n",
    "    contour_item = ax.contour(x_range, y_range, con, 0, linestyles='dashed', colors=color_)\n",
    "    contour_item.levels = [label_ for val in contour_item.levels]\n",
    "    ax.clabel(contour_item, contour_item.levels, inline=True, fontsize=20, colors='black')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Mahalanobis distance\n",
    "For classes `a` and `b`, give the expression of the Mahalanobis distance used to classify a point in class `a` or `b`, and verify the obtained classification, in comparison with the \"complete\" Bayes classification, for a few points of the plane.\n",
    "\n",
    "According to the Bayesian classification for normal laws, if the covariance matrix $\\Sigma$ is not diagoal, the most probable class is the one which minimizes the Mahalanobis distance.\n",
    "\n",
    "Mahalanobis Distance:\n",
    "\n",
    "$d_m = \\left((x-\\mu_i)\\Sigma^{-1}(x-\\mu_i)\\right)^{1/2}$\n",
    "\n",
    "the class determined by Mahalanobis distance according to below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class=\n",
    "$\n",
    "\\left\\{\n",
    "\\begin{array}{rcl}\n",
    "a & & {d_a <= d_b}\\\\\n",
    "b & & {d_a > d_b}\n",
    "\\end{array} \\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mahalanobis distance of a data point from a dataset according to the equation above\n",
    "def get_mahalanobis_distance(data, new_point):\n",
    "    mean , sig = get_coefficient(data)\n",
    "    inv_sig = np.linalg.inv(sig)\n",
    "    d_m = np.sqrt((new_point - mean).dot(inv_sig).dot((new_point - mean).T))\n",
    "    return d_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that reflect the performance of method of Mahalanobis distance\n",
    "# input: dataset and label\n",
    "# output: classification accuracy, list of data points that are correctly classified \n",
    "# and list of data points that are wrongly classified\n",
    "def mahalanobis_distance_performance(new_data, label):\n",
    "    total_num = len(new_data)\n",
    "    correct_num = 0\n",
    "    correct_list = []\n",
    "    wrong_list = []\n",
    "    \n",
    "    for data_ in new_data:\n",
    "        # computation of mahalanobis distance between data point and points of class a and b\n",
    "        temp_dm_a = get_mahalanobis_distance(a, data_)\n",
    "        temp_dm_b = get_mahalanobis_distance(b, data_)\n",
    "        \n",
    "        # classification by minimizing the mahalanobis distance\n",
    "        prediction = np.argmin([temp_dm_a, temp_dm_b], axis = 0)\n",
    "        \n",
    "        # check if the classisification is correct\n",
    "        if prediction == label:\n",
    "            correct_num += 1\n",
    "            correct_list.append(data_)\n",
    "        else:\n",
    "            wrong_list.append(data_)\n",
    "            \n",
    "    accuracy=correct_num/total_num\n",
    "    \n",
    "    return accuracy, np.array(correct_list), np.array(wrong_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that reflect the performance of method of complete bayesian\n",
    "# input: dataset and label\n",
    "# output: classification accuracy, list of data points that are correctly classified \n",
    "# and list of data points that are wrongly classified\n",
    "def complete_bayesian_performance(new_data, label):\n",
    "    total_num=len(new_data)\n",
    "    correct_num=0\n",
    "    correct_list=[]\n",
    "    wrong_list=[]\n",
    "    \n",
    "    for data_ in new_data:\n",
    "        temp_x = data_[0]\n",
    "        temp_y = data_[1]\n",
    "        \n",
    "        # bayesian classification \n",
    "        prediction = get_curve(coeff_ab, temp_x, temp_y)>0\n",
    "        \n",
    "        # check if the classisification is correct\n",
    "        if prediction == label:\n",
    "            correct_num+=1\n",
    "            correct_list+= [data_]\n",
    "        else:\n",
    "            wrong_list += [data_]\n",
    "    accuracy=correct_num/total_num\n",
    "    return accuracy, np.array(correct_list), np.array(wrong_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the classification using the Mahalanobis distance compared to the Bayesian classification\n",
    "# Use of letters a and b training set to test both classification algorithm\n",
    "accuracy_a_mah, a_correct_list_mah, a_wrong_list_mah = mahalanobis_distance_performance(a, 0)\n",
    "accuracy_b_mah, b_correct_list_mah, b_wrong_list_mah = mahalanobis_distance_performance(b, 1)\n",
    "\n",
    "accuracy_a_bayes, a_correct_list_bayes, a_wrong_list_bayes = complete_bayesian_performance(a, 0)\n",
    "accuracy_b_bayes, b_correct_list_bayes, b_wrong_list_bayes = complete_bayesian_performance(b, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparaison of with Mahalanobis distance and Bayesian Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of the points of classes a and b correctly/wrongly classified using the mahalanobis distance method\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,6))\n",
    "ax[0].scatter(a_correct_list_mah[:, 0], a_correct_list_mah[:, 1], label = 'correctly classified a')\n",
    "ax[0].scatter(a_wrong_list_mah[:, 0], a_wrong_list_mah[:, 1], label = 'prediction:b,  ground truth:a')\n",
    "ax[0].scatter(b_correct_list_mah[:, 0], b_correct_list_mah[:, 1], label = 'correctly classified b')\n",
    "ax[0].scatter(b_wrong_list_mah[:, 0], b_wrong_list_mah[:, 1], label = 'prediction:a,  ground truth:b')\n",
    "ax[0].set_title(\"Classification Performance by Mahalanobis Distance\")\n",
    "\n",
    "# plot of the points of classes a and b correctly/wrongly classified using the bayesian method\n",
    "ax[1].scatter(a_correct_list_bayes[:, 0], a_correct_list_bayes[:, 1], label = 'correctly classified a')\n",
    "ax[1].scatter(a_wrong_list_bayes[:, 0], a_wrong_list_bayes[:, 1], label = 'prediction:b,  ground truth:a')\n",
    "ax[1].scatter(b_correct_list_bayes[:, 0], b_correct_list_bayes[:, 1], label = 'correctly classified b')\n",
    "ax[1].scatter(b_wrong_list_bayes[:, 0], b_wrong_list_bayes[:, 1], label = 'prediction:a,  ground truth:b')\n",
    "contour_item = ax[1].contour(x_range, y_range, curve_ab, 0,linestyles='dotted', colors='black')\n",
    "contour_item.levels = [' AB Separation' for val in contour_item.levels]\n",
    "ax[1].clabel(contour_item, contour_item.levels, inline=True, fontsize=10, colors='black')\n",
    "ax[1].set_title(\"Classification Performance by Bayes Method\")\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the accuracy of both methods for comparison\n",
    "print('By Bayes Method') \n",
    "print('Accuracy for a data set = {}'.format(accuracy_a_bayes))\n",
    "print('Accuracy for b data set = {}'.format(accuracy_b_bayes))\n",
    "print('Accuracy for all data in a and b data sets = {:2f}'.format(0.5 * (accuracy_a_bayes + accuracy_b_bayes)))\n",
    "print('-----------------------------------------------------------------')\n",
    "print('By Mahalanobis Distance')\n",
    "print('Accuracy for a data set = {}'.format(accuracy_a_mah))\n",
    "print('Accuracy for b data set = {}'.format(accuracy_b_mah))\n",
    "print('Accuracy for all data in a and b data sets = {:2f}'.format(0.5 * (accuracy_a_mah + accuracy_b_mah)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the Bayesian classification for normal laws, if the covariance matrix $\\Sigma$ is not diagonal, the most probable class is the one which minimizes the Mahalanobis distance.\n",
    "\n",
    "The plots of classification and numerical accuracy show that mahalanobis distance and complete Bayesian classifier share the same result and performance in this case, which proves the theoretical hypothesis, along with the same prior probabilities of a and b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "In this part, we aim to classify digits using the complete version of MNIST digits dataset.\n",
    "The dataset consists of 60'000 training images and 10'000 test images of handwritten digits.\n",
    "Each image has size 28x28, and has assigned a label from zero to nine, denoting the digits value.\n",
    "Given this data, your task is to construct a Multilayer Perceptron (MLP) for supervised training and classification and evaluate it on the test images.\n",
    "\n",
    "Download the MNIST dataset (all 4 files) from http://yann.lecun.com/exdb/mnist/ under `lab-03-data/part2`.\n",
    "You can then use the script provided below to extract and load training and testing images in Python.\n",
    "\n",
    "To create an MLP you are free to choose any library.\n",
    "In case you don't have any preferences, we encourage you to use the [scikit-learn] package; it is a simple, efficient and free tool for data analysis and machine learning.\n",
    "In this [link][sklearn-example], you can find a basic example to see how to create and train an MLP using [scikit-learn].\n",
    "Your network should have the following properties:\n",
    "* Input `x`: 784-dimensional (i.e. 784 visible units representing the flattened 28x28 pixel images).\n",
    "* 100 hidden units `h`.\n",
    "* 10 output units `y`, i.e. the labels, with a value close to one in the i-th class representing a high probability of the input representing the digit `i`.\n",
    "\n",
    "If you need additional examples you can borrow some code from image classification tutorials.\n",
    "However, we recommend that you construct a minimal version of the network on your own to gain better insights.\n",
    "\n",
    "[scikit-learn]: http://scikit-learn.org/stable/index.html\n",
    "[sklearn-example]: http://scikit-learn.org/stable/modules/neural_networks_supervised.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset loading\n",
    "Here we first declare the methods `extract_data` and `extract_labels` so that we can reuse them later in the code.\n",
    "Then we extract both the data and corresponding labels, and plot randomly some images and corresponding labels of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def extract_data(filename, image_shape, image_number):\n",
    "    '''extract the data from a file '''\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(np.prod(image_shape) * image_number)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = data.reshape(image_number, image_shape[0], image_shape[1])\n",
    "    return data\n",
    "\n",
    "def extract_labels(filename, image_number):\n",
    "    '''extract the labels from a file'''\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * image_number)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__\n",
    "tf_version, _, _ = tf.__version__.split(\".\")\n",
    "\n",
    "if int(tf_version) < 2:\n",
    "    print(\"Please updgrade your version of tensorflow by using following command `pip install --upgrade tensorflow`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the openened files\n",
    "image_shape = (28, 28)\n",
    "train_set_size = 60000\n",
    "test_set_size = 10000\n",
    "\n",
    "# defining the files path\n",
    "data_base_path = os.path.join(os.pardir, 'data')\n",
    "data_folder = 'lab-03-data'\n",
    "data_part2_folder = os.path.join(data_base_path, data_folder, 'part2')\n",
    "\n",
    "train_images_path = os.path.join(data_part2_folder, 'train-images-idx3-ubyte.gz')\n",
    "train_labels_path = os.path.join(data_part2_folder, 'train-labels-idx1-ubyte.gz')\n",
    "test_images_path = os.path.join(data_part2_folder, 't10k-images-idx3-ubyte.gz')\n",
    "test_labels_path = os.path.join(data_part2_folder, 't10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "#extract data and labels from files\n",
    "train_images_extract = extract_data(train_images_path, image_shape, train_set_size)\n",
    "test_images_extract = extract_data(test_images_path, image_shape, test_set_size)\n",
    "train_labels_extract = extract_labels(train_labels_path, train_set_size)\n",
    "test_labels_extract = extract_labels(test_labels_path, test_set_size)\n",
    "\n",
    "print(\"Training Set \", train_images_extract.shape, train_labels_extract.shape)\n",
    "print(\"Test Set\", test_images_extract.shape, test_labels_extract.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prng = np.random.RandomState(seed=123456789)  # seed to always re-draw the same distribution\n",
    "# choose randomly 10 indexes among all images indexes\n",
    "plt_ind = prng.randint(low=0, high=train_set_size, size=10)\n",
    "\n",
    "# plot the 10 chosen images\n",
    "fig, axes = plt.subplots(1, 10, figsize=(12, 3))\n",
    "for ax, im, lb in zip(axes, train_images_extract[plt_ind], train_labels_extract[plt_ind]):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "def reformat(dataset, labels):\n",
    "    \"\"\"\n",
    "        Reformat the data to the one-hot and flattened mode\n",
    "    \"\"\"\n",
    "    n_dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "\n",
    "    # Convert to the one hot format\n",
    "    n_labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "\n",
    "    return n_dataset, n_labels\n",
    "\n",
    "\n",
    "num_labels = 10\n",
    "\n",
    "train_images, train_labels = reformat(train_images_extract, train_labels_extract)\n",
    "test_images, test_labels = reformat(test_images_extract, test_labels_extract)\n",
    "\n",
    "# Display the files\n",
    "print(\"Training Set - shape: \", train_images.shape, train_labels.shape)\n",
    "print(\"Test Set - shape: \", test_images.shape, test_labels.shape)\n",
    "print(\"Test Set labels - [1 to 5]: \\n\", test_labels[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "learning_rate = 0.01\n",
    "num_epoch = 11\n",
    "display_step = 5       # display the avg cost after this number of epochs\n",
    "\n",
    "# Variables\n",
    "batch_size = 100\n",
    "num_input = 784 # image size as array\n",
    "num_hidden1 = 100\n",
    "num_hidden2 = 100\n",
    "num_output = 10 #0-9 digit recognition = 10 labels\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_layer_perceptron(x, weights, biases):\n",
    "    \"\"\"\n",
    "    MLP model with 2 hidden layers.\n",
    "    \"\"\"\n",
    "    hidden_layer1 = tf.add(tf.matmul(x, weights['w_h1']), biases['b_h1'])\n",
    "    hidden_layer1 = tf.nn.relu(hidden_layer1)   # apply ReLU non-linearity\n",
    "    hidden_layer2 = tf.add(tf.matmul(hidden_layer1, weights['w_h2']), biases['b_h2'])\n",
    "    hidden_layer2 = tf.nn.relu(hidden_layer2)\n",
    "\n",
    "    out_layer = tf.add(tf.matmul(hidden_layer2, weights['w_out']), biases['b_out'])  # NO non-linearity in the output layer\n",
    "\n",
    "\n",
    "    return out_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.InteractiveSession()\n",
    "try:\n",
    "    # Trainum_input data and labels\n",
    "    x = tf.placeholder('float', [None, num_input])     # training data\n",
    "    y = tf.placeholder('float', [None, num_output])    # labels\n",
    "\n",
    "    # Weights and biases\n",
    "    weights = {\n",
    "        'w_h1' : tf.Variable(tf.random_normal([num_input, num_hidden1])),\n",
    "        'w_h2' : tf.Variable(tf.random_normal([num_hidden1, num_hidden2])),\n",
    "        'w_out': tf.Variable(tf.random_normal([num_hidden2, num_output]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b_h1' : tf.Variable(tf.random_normal([num_hidden1])), \n",
    "        'b_h2' : tf.Variable(tf.random_normal([num_hidden2])),\n",
    "        'b_out': tf.Variable(tf.random_normal([num_output]))\n",
    "    }\n",
    "\n",
    "    # Construct the model\n",
    "    model = multi_layer_perceptron(x, weights, biases)\n",
    "\n",
    "    # Cost function and optimization\n",
    "    # Cost function: measures the probability error in discrete classification, classes being mutually exclusive\n",
    "    loss_func = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = model, labels = y))\n",
    "    # Adam optimizer: stochastic gradient descent method, based on adaptive estimation of 1st/2nd-order moments\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss_func)\n",
    "\n",
    "    # Train and test\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    cost_all = np.array([])\n",
    "    acc_all = np.array([])\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Start training Multi Layer Perceptron model...\")\n",
    "    # go through the training set 'num_epoch' times\n",
    "    for iter in range(num_epoch):\n",
    "        avg_cost = 0.0\n",
    "        num_batch = train_images.shape[0] // batch_size   # total number of batches\n",
    "        # loop over all batches\n",
    "        for nB in range(num_batch): \n",
    "            # for each batch, definition of the training set (data, labels)\n",
    "            trainData = train_images[nB*batch_size:(nB+1)*batch_size]\n",
    "            trainLabels = train_labels[nB*batch_size:(nB+1)*batch_size]\n",
    "            tmp_cost, _ = sess.run([loss_func, optimizer], feed_dict={x: trainData, y: trainLabels})\n",
    "\n",
    "            avg_cost = avg_cost + tmp_cost / num_batch\n",
    "        \n",
    "        # Test the model: prediction and accuracy\n",
    "        correct_pred = tf.equal(tf.arg_max(model, 1), tf.arg_max(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, 'float'))\n",
    "        acc = accuracy.eval(session = sess, feed_dict = {x: test_images, y: test_labels})\n",
    "\n",
    "        if iter % display_step == 0:\n",
    "            print('Epoch: %02d' %(iter), 'cost: ' + \"{:.2f}\".format(avg_cost), 'accuracy: ' + \"{:.3f}\".format(acc))\n",
    "        cost_all = np.append(cost_all, avg_cost)\n",
    "        acc_all = np.append(acc_all, acc)\n",
    "    print(\"Training done...\")\n",
    "\n",
    "\n",
    "    # plot the accuracy and loss\n",
    "    plt.figure(figsize = (15,5))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(range(num_epoch), cost_all, color='r')\n",
    "    plt.subplot(122)\n",
    "    plt.plot(range(num_epoch), acc_all)\n",
    "    plt.ylim(0,1)\n",
    "    plt.show()\n",
    "    \n",
    "    final_pred = tf.arg_max(model, 1)\n",
    "    prediction = final_pred.eval(session = sess, feed_dict = {x: test_images})\n",
    "except:\n",
    "    print('Error')\n",
    "\n",
    "finally:\n",
    "    s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prng = np.random.RandomState()\n",
    "plt_ind = prng.randint(low=0, high=test_set_size, size=10)\n",
    "\n",
    "# plot of 10 random images and of their class prediction\n",
    "fig, axes = plt.subplots(1, 10, figsize=(20, 20))\n",
    "for ax, im, lb, pred in zip(axes, test_images_extract[plt_ind], test_labels_extract[plt_ind], prediction[plt_ind]):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(\"Pred:{} (True:{})\".format(pred,lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to implement 2 hidden layers of 100 nodes because the results were not satisfying with one hidden layer of 100 nodes. \n",
    "As we can see, the resukts are pretty satisfying as the final accuracy is higher than 90% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
